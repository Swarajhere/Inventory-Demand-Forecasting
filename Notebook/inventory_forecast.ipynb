{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ded31c-6e96-4c5e-ae6b-099efcb836bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preprocessing data...\n",
      "Preprocessing done in 3.59s — dropped 15000 rows (NaNs).\n",
      "Data shape after preprocessing: (898000, 14)\n",
      "Train shape: (852000, 12), Validation shape: (46000, 12)\n",
      "\n",
      "Training RandomForestRegressor...\n",
      "RandomForest done in 158.29s\n",
      "--- RandomForest Evaluation ---\n",
      "RMSE: 8.2027\n",
      "MAE : 6.2737\n",
      "\n",
      "\n",
      "Training XGBoost (robust fallback implementation)...\n",
      "XGBRegressor.fit(...) refused early stopping args — retrying without them.\n",
      "XGBoost done in 71.12s\n",
      "--- XGBoost Evaluation ---\n",
      "RMSE: 7.6418\n",
      "MAE : 5.9015\n",
      "\n",
      "\n",
      "Training LightGBMRegressor...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1254\n",
      "[LightGBM] [Info] Number of data points in the train set: 852000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 52.522494\n",
      "LightGBM done in 42.44s\n",
      "--- LightGBM Evaluation ---\n",
      "RMSE: 7.6527\n",
      "MAE : 5.9104\n",
      "\n",
      "\n",
      "--- Model Performance Summary ---\n",
      "                  RMSE       MAE\n",
      "XGBoost       7.641782  5.901464\n",
      "LightGBM      7.652698  5.910434\n",
      "RandomForest  8.202739  6.273742\n",
      "Best model by RMSE: XGBoost\n",
      "\n",
      "Generating visualizations for the best model and feature importances...\n",
      "Saved plot: XGBoost_actual_vs_predicted_store1_item1.png\n",
      "Saved feature importance: XGBoost_feature_importance.png\n",
      "Saved feature importance: LightGBM_feature_importance.png\n",
      "Saved feature importance: RandomForest_feature_importance.png\n",
      "\n",
      "All done. Artifacts (plots) saved in the current directory.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Inventory Demand Forecasting Project\n",
    "\n",
    "This script builds a complete machine learning pipeline to forecast sales demand\n",
    "for different store-item combinations.\n",
    "\n",
    "It covers the following steps:\n",
    "1.  Data Loading and Preprocessing: Loads the dataset and creates time-based,\n",
    "    lag, and rolling window features.\n",
    "2.  Train/Validation Split: Splits the data into training and validation sets\n",
    "    based on a time threshold.\n",
    "3.  Model Training: Implements and trains three different regression models:\n",
    "    - RandomForestRegressor\n",
    "    - XGBRegressor (with a robust, version-agnostic failsafe)\n",
    "    - LGBMRegressor (with early stopping)\n",
    "4.  Model Evaluation: Evaluates the models on the validation set using\n",
    "    Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
    "5.  Visualization: Plots actual vs. predicted sales and feature importance\n",
    "    for the boosting models.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# XGBoost imports (both high-level wrapper and low-level API)\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# LightGBM imports\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm.callback import early_stopping as lgb_early_stopping\n",
    "\n",
    "# --- Configuration & Styling ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "\n",
    "def load_and_preprocess(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads data and performs preprocessing and feature engineering.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Loading and preprocessing data...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, parse_dates=['date'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {filepath} was not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort for proper lag/rolling creation\n",
    "    df.sort_values(by=['store', 'item', 'date'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Time-based features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # Lag features (within each store-item series)\n",
    "    grouped = df.groupby(['store', 'item'])['sales']\n",
    "    for lag in (7, 14, 30):\n",
    "        df[f'sales_lag_{lag}'] = grouped.shift(lag)\n",
    "\n",
    "    # Rolling mean features (shifted so they don't include current day)\n",
    "    for window in (7, 30):\n",
    "        df[f'sales_rollmean_{window}'] = grouped.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    # Drop rows with NaNs introduced by lags/rolls\n",
    "    before = df.shape[0]\n",
    "    df.dropna(inplace=True)\n",
    "    after = df.shape[0]\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Preprocessing done in {end_time - start_time:.2f}s — dropped {before - after} rows (NaNs).\")\n",
    "    print(f\"Data shape after preprocessing: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):\n",
    "    \"\"\"\n",
    "    Evaluate and print RMSE and MAE for predictions.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"--- {model_name} Evaluation ---\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE : {mae:.4f}\\n\")\n",
    "    return {'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(df_val: pd.DataFrame, y_pred: np.ndarray, model_name: str,\n",
    "                             store_id: int = 1, item_id: int = 1):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted sales for a chosen store-item pair.\n",
    "    y_pred must be aligned (same order) as df_val.\n",
    "    \"\"\"\n",
    "    dfp = df_val.copy()\n",
    "    dfp['predicted_sales'] = np.array(y_pred).flatten()\n",
    "    mask = (dfp['store'] == store_id) & (dfp['item'] == item_id)\n",
    "    sub = dfp.loc[mask]\n",
    "    if sub.empty:\n",
    "        print(f\"No data for store={store_id}, item={item_id} in validation set — skipping plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(sub['date'], sub['sales'], label='Actual', linewidth=2)\n",
    "    plt.plot(sub['date'], sub['predicted_sales'], label='Predicted', linestyle='--', linewidth=2)\n",
    "    plt.title(f\"{model_name}: Actual vs Predicted (Store {store_id}, Item {item_id})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    filename = f\"{model_name}_actual_vs_predicted_store{store_id}_item{item_id}.png\"\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {filename}\")\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, features: list, model_name: str):\n",
    "    \"\"\"\n",
    "    Plot feature importances. Supports sklearn-style models (feature_importances_),\n",
    "    LightGBM (sklearn wrapper), and xgboost.Booster (low-level).\n",
    "    \"\"\"\n",
    "    # sklearn-style (RandomForest, LGBM, XGBRegressor)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        imp = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False).head(15)\n",
    "    else:\n",
    "        # xgboost Booster fallback\n",
    "        try:\n",
    "            # Booster object\n",
    "            if isinstance(model, xgb.core.Booster) or model.__class__.__name__ == 'Booster':\n",
    "                score = model.get_score(importance_type='gain')  # keys like 'f0', 'f1' or feature names\n",
    "                # Map to feature names robustly\n",
    "                imp_map = {f: 0.0 for f in features}\n",
    "                for k, v in score.items():\n",
    "                    if k.startswith('f'):\n",
    "                        # 'f{index}' -> index position in features\n",
    "                        try:\n",
    "                            idx = int(k[1:])\n",
    "                            if 0 <= idx < len(features):\n",
    "                                imp_map[features[idx]] = v\n",
    "                        except ValueError:\n",
    "                            # unexpected formatting; ignore\n",
    "                            pass\n",
    "                    else:\n",
    "                        # If key is actual feature name\n",
    "                        if k in imp_map:\n",
    "                            imp_map[k] = v\n",
    "                imp = pd.Series(imp_map).sort_values(ascending=False).head(15)\n",
    "            else:\n",
    "                print(f\"Model type {type(model)} not supported for feature importance plotting.\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting feature importance:\", e)\n",
    "            return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=imp.values, y=imp.index)\n",
    "    plt.title(f\"Top Feature Importances: {model_name}\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    fname = f\"{model_name}_feature_importance.png\"\n",
    "    plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved feature importance: {fname}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load & preprocess\n",
    "    data = load_and_preprocess('train.csv')\n",
    "    if data.empty:\n",
    "        return\n",
    "\n",
    "    # Time-based train/validation split (last 3 months of 2017 as validation)\n",
    "    train_df = data[data['date'] < '2017-10-01'].copy()\n",
    "    val_df = data[data['date'] >= '2017-10-01'].copy()\n",
    "\n",
    "    # Features & target\n",
    "    features = [c for c in data.columns if c not in ['date', 'sales']]\n",
    "    target = 'sales'\n",
    "\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[target]\n",
    "    X_val = val_df[features]\n",
    "    y_val = val_df[target]\n",
    "\n",
    "    print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "    models = {}\n",
    "    predictions = {}\n",
    "    results = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # Random Forest (baseline)\n",
    "    # -----------------------------\n",
    "    print(\"\\nTraining RandomForestRegressor...\")\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=12,\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_val)\n",
    "    t1 = time.time()\n",
    "    print(f\"RandomForest done in {t1 - t0:.2f}s\")\n",
    "    models['RandomForest'] = rf\n",
    "    predictions['RandomForest'] = rf_pred\n",
    "    results['RandomForest'] = evaluate_model(y_val, rf_pred, 'RandomForest')\n",
    "\n",
    "    # -----------------------------\n",
    "    # XGBoost (robust, failsafe)\n",
    "    # -----------------------------\n",
    "    print(\"\\nTraining XGBoost (robust fallback implementation)...\")\n",
    "    t0 = time.time()\n",
    "    xgb_preds = None\n",
    "    xgb_model = None\n",
    "\n",
    "    xgb_params = dict(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Try high-level sklearn API with early_stopping_rounds first (may or may not be accepted)\n",
    "    try:\n",
    "        xgb_model = XGBRegressor(**xgb_params)\n",
    "        try:\n",
    "            # Preferred: ask sklearn wrapper for early stopping & eval_metric (works on many versions)\n",
    "            xgb_model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=20,\n",
    "                eval_metric=\"rmse\",\n",
    "                verbose=False\n",
    "            )\n",
    "            xgb_preds = xgb_model.predict(X_val)\n",
    "            print(\"XGBRegressor.fit(...) succeeded with early stopping.\")\n",
    "        except TypeError:\n",
    "            # Some xgboost versions raise TypeError when early_stopping_rounds / eval_metric passed here.\n",
    "            # Retry without early stopping.\n",
    "            print(\"XGBRegressor.fit(...) refused early stopping args — retrying without them.\")\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_preds = xgb_model.predict(X_val)\n",
    "    except Exception as high_err:\n",
    "        # If sklearn wrapper completely fails, fall back to low-level xgb.train API using DMatrix.\n",
    "        print(\"High-level XGBRegressor failed:\", repr(high_err))\n",
    "        print(\"Falling back to low-level xgboost.train() with DMatrix and early stopping.\")\n",
    "        try:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val, feature_names=features)\n",
    "            params = {\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"eta\": 0.05,\n",
    "                \"max_depth\": 7,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"eval_metric\": \"rmse\",\n",
    "                \"seed\": 42\n",
    "            }\n",
    "            watchlist = [(dtrain, \"train\"), (dval, \"eval\")]\n",
    "            booster = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=watchlist,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            xgb_model = booster  # note: Booster object (not sklearn wrapper)\n",
    "            xgb_preds = booster.predict(dval)\n",
    "            print(\"Low-level xgboost.train() completed with early stopping.\")\n",
    "        except Exception as low_err:\n",
    "            print(\"Low-level xgboost.train() also failed:\", repr(low_err))\n",
    "            raise RuntimeError(\"XGBoost training failed in both sklearn wrapper and low-level API.\") from low_err\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"XGBoost done in {t1 - t0:.2f}s\")\n",
    "    models['XGBoost'] = xgb_model\n",
    "    predictions['XGBoost'] = np.array(xgb_preds).flatten()\n",
    "    results['XGBoost'] = evaluate_model(y_val, predictions['XGBoost'], 'XGBoost')\n",
    "\n",
    "    # -----------------------------\n",
    "    # LightGBM\n",
    "    # -----------------------------\n",
    "    print(\"\\nTraining LightGBMRegressor...\")\n",
    "    t0 = time.time()\n",
    "    lgb = LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        max_depth=-1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Use callback early stopping if supported\n",
    "    try:\n",
    "        lgb.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[lgb_early_stopping(stopping_rounds=20, verbose=False)],\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Some versions accept early_stopping_rounds directly instead of callbacks\n",
    "        print(\"LightGBM.fit(...) refused callbacks argument — retrying with early_stopping_rounds.\")\n",
    "        lgb.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=20\n",
    "        )\n",
    "    lgb_preds = lgb.predict(X_val)\n",
    "    t1 = time.time()\n",
    "    print(f\"LightGBM done in {t1 - t0:.2f}s\")\n",
    "    models['LightGBM'] = lgb\n",
    "    predictions['LightGBM'] = np.array(lgb_preds).flatten()\n",
    "    results['LightGBM'] = evaluate_model(y_val, predictions['LightGBM'], 'LightGBM')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compare results\n",
    "    # -----------------------------\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\n--- Model Performance Summary ---\")\n",
    "    print(results_df.sort_values(by='RMSE'))\n",
    "\n",
    "    best_model_name = results_df['RMSE'].idxmin()\n",
    "    print(f\"Best model by RMSE: {best_model_name}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Visualizations\n",
    "    # -----------------------------\n",
    "    print(\"\\nGenerating visualizations for the best model and feature importances...\")\n",
    "    # Plot actual vs predicted for a sample store-item\n",
    "    plot_actual_vs_predicted(val_df, predictions[best_model_name], best_model_name, store_id=1, item_id=1)\n",
    "\n",
    "    # Feature importance plots for boosting models (if available)\n",
    "    plot_feature_importance(models['XGBoost'], features, 'XGBoost')\n",
    "    plot_feature_importance(models['LightGBM'], features, 'LightGBM')\n",
    "\n",
    "    # Optionally also show RF importance\n",
    "    plot_feature_importance(models['RandomForest'], features, 'RandomForest')\n",
    "\n",
    "    print(\"\\nAll done. Artifacts (plots) saved in the current directory.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d359a-e6ad-4165-959c-46b75015c49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
